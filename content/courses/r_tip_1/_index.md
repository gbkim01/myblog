---
title: 개요
author: 김강배
date: '2020-08-01'
categories:
  - class
linktitle: '2020 08 01 '
lastmod: '2020-08-01T20:39:08Z'
toc: true
type: docs
menu:
  r_tip_1:
    name: 개요
    weight: 1
---
![](https://user-images.githubusercontent.com/30010992/89122878-fe867980-d505-11ea-812f-50da4890223b.png)

---

## **정리에 앞서...**

우렁각시가 일목요연하게 정리된 데이터를 눈 앞에 "뿅" 하고 바치는,  아름다운 상상을 연구자라면 누구나 한번쯤 해봤을 것입니다. 그런데 실상은 정리된 자료는 커녕 자료의 유무조차 불확실한 경우가 허다합니다. 없는 데이터를 어떻게든 스스로 구축하려고 노력하지만, 무에서 유를 창조하는 일 만큼이나 데이터를 수집하는 행위 역시 개인이 감당하기에는 무척이나 벅찬 일 입니다. 게다가 이러한 과정은 필수적으로 압도적인 분량의 단순 사무노동을 수반하기 때문에 대부분의 사람들은 자료 분석 보다도 자료의 취합에서 부터 질리게 마련입니다.

만능은 아니지만, **인터넷**은 현실적으로 가장 쉽게 자료를 획득할 수 있는 수단이 될 수 있습니다. 인터넷에는 우리가 생각하는 것 이상으로 많은 정보가 부유하고 있습니다. 더군다나 비정형 데이터에 대한 분석기법이 발달하면서 우리가 사용할 수 있는 인터넷 자료의 유형에 제한은 없다시피 합니다. 예컨대
소셜미디어의 자잘한 글에서부터 언론기사와  공공기관의 공개자료에 이르기까지, 인터넷에서 획득할 수 있는 거의 모든 유형의 자료들로부터 유의미한 분석결과를 도출할 수 있습니다. 

여기 **Web Crawling with R**에서는 급하게 써먹을 수 있는 크롤링 방법을 정리합니다. 실제 현업에서 전문적으로 하시는 분들이 본다면 콧 웃음 칠 내용이겠지만, 지금까지의 개인적인 경험을 바탕으로 차근차근 정리할 요량입니다.  



## **정리의 순서**

정리의 순서는 다음과 같은 보편적인 크롤링의 순서를 따라갈 것입니다.

1. 크롤링 하고자 하는 대상 확인
2. 웹페이지의 요청과 응답
3. 내려 받은 웹 문서 파일을 분석하고 필요한 자료 수집
4. 여러 웹페이지를 대상으로 2와 3의 단계를 반복하며 데이터 수집/정리
