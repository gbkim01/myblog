---
title: 개요
author: 김강배
date: '2020-08-01'
categories:
  - class
linktitle: '2020 08 01 '
lastmod: '2020-08-01T20:39:08Z'
toc: true
type: docs
menu:
  r_tip_1:
    name: 개요
    weight: 1
---
![](https://user-images.githubusercontent.com/30010992/89122878-fe867980-d505-11ea-812f-50da4890223b.png)

---

## **정리에 앞서...**

우렁각시라도 있어서 자료정리는 다른 사람이 해주고, 자기는 일목요연하게 정리된 데이터를 자리에 앉아 받기만 해도 되는 상상을 한번쯤 해봤을 겁니다. 단순 반복작업일지라도 자료정리에 투입되는 시간과 노력이 어마무시하기 때문에 웬만하면 본인은 그런 고통을 피하고 싶은 희망사항이겠지요. 그런데 실상은 정리된 자료는 커녕 자료의 존재유무조차 불확실한 경우가 허다합니다. 수일을 고민하고 머리를 쥐어 짜내서 겨우 아이디어 하나를 만들었는데, 뒷받침할 근거가 없어 그대로 사장시킬수 밖에 없는 상황이 되는 겁니다. 

만능은 아니더라도 인터넷에서 자료를 대량으로 긁어오는 **크롤링 작업**은 이럴때 큰 도움이 됩니다. **인터넷**은 우리가 생각하는 것 이상으로 많은 정보가 부유하고 있기 때문에 현실적으로 가장 쉽게 접근할 수 있는 자료 풀(pool)입니다. 더군다나 비정형 데이터에 대한 분석기법이 발달하면서 우리가 사용할 수 있는 자료 유형에 대한 제약이 사라짐에 따라 인터넷 자료의 활용범위는 크게 확장되었습니다. 예컨대 공공기관의 공신력 있는 공개자료 부터 언론기사와 소셜미디어의 자잘한 글, 그리고 각종 이미지 파일에 이르기까지... 분석기법에 대한 지식만 있다면 인터넷에서 획득할 수 있는 거의 모든 유형의 자료로부터 유의미한 결과를 도출할 수 있습니다. 

여기 **Web Crawling with R**에서는 급하게 써먹을 수 있는 크롤링 방법을 정리합니다. 단순히 웹에서 자료를 내려받는 수준의 **"이런 것도 크롤링인가?"** 싶은 작업을 R을 통해 더 쉽게 할 수 있는 방법 또한 정리합니다. 비전공자의 설명이라 실제 현업에서 전문적으로 하시는 분들이 본다면 콧 웃음 칠 내용이겠지만, 지금까지의 개인적인 경험을 바탕으로 차근차근 작성할 요량입니다.  



## **정리의 순서**

정리의 순서는 다음과 같은 보편적인 크롤링의 순서를 따라갈 것입니다.

1. 크롤링 하고자 하는 대상 식별
2. 웹페이지의 요청과 응답
3. 내려 받은 웹 문서 파일을 분석하고 필요한 자료 수집
4. 여러 웹페이지를 대상으로 2와 3의 단계를 반복하며 데이터 수집/정리
