---
title: 개요
author: 김강배
date: '2020-08-01'
categories:
  - class
linktitle: '2020 08 01 '
lastmod: '2020-08-01T20:39:08Z'
toc: true
type: docs
menu:
  r_tip_1:
    name: 개요
    weight: 1
---
![](https://user-images.githubusercontent.com/30010992/89122878-fe867980-d505-11ea-812f-50da4890223b.png)

---

## **정리에 앞서...**

데이터 분석은 보통 다음의 세가지 단계를 거쳐 진행됩니다.

1. 자료의 취합과 정리
2. 자료 분석 및 가설검증
3. 분석결과의 정리 및 보고 

우리가 알고자 하는 분야에 대해서 딱 들어맞는 자료가 일목요연하게 정리되어 내 눈 앞에 있으면 좋으련만, 보통의 경우에는 그렇지 못한게 현실이기 때문에 흔히들 데이터 분석의 1단계에서 부터 거대한 장벽을 느끼게 됩니다. 그래도 어쩌겠습니까? 목마른 자가 우물을 파듯이 대부분 직접 데이터를 구축해야 하는데, 현실세계에서 데이터를 수집하는 것은 생각보다 훨씬 많은 자원이 필요하고 개인이 감당하기에는 매우 벅찬 일입니다. 

가령 100명을 대상으로 설문조사를 진행한다고 가정해 봅시다. 대상인원을 선정하고 대상인원 마다 개별적으로 접근해서 질문에 대한 답을 얻어야 합니다. 더군다나 100명의 대상에게 요청해서 100개의 답을 얻으리라는 보장도 없습니다. 때때로 우리는 100개의 설문 응답을 얻기 위해 300명, 500명의 사람에게 답변을 요청해야 할 수도 있습니다. 

그래서 우리는 현실적으로 가장 쉽게 자료를 획득할 수 있는 수단으로써 **웹**에 관심을 둘 수 밖에 없습니다.  인터넷에는 우리가 생각하는 것 보다 훨씬 많은 자료들이 존재합니다. 소셜미디어의 자잘한 글에서 부터 시작해서 언론기사와  공공기관이 발표하는 공개자료에 이르기 까지, 인터넷에 떠도는 자료들을 모으는 것은 데이터 구축의 현실적인 대안입니다. 그리고 **웹**이라는 바다에서 필요한 정보를 수집함에 있어 크롤링은 정말 유용한 수단이 됩니다.

여기 **Web Crawling with R**에서는 급하게 써먹을 수 있는 크롤링 방법을 정리합니다. 실제 현업에서 전문적으로 하시는 분들이 본다면 콧웃음 칠 내용이겠지만, 코딩과 무관한 삶을 살았던 사람들에게는 큰 도움이 될 것입니다. 



## **정리의 순서**

정리의 순서는 다음과 같은 보편적인 크롤링의 순서를 따라갈 것입니다.

1. 크롤링 하고자 하는 대상 확인
2. 웹 서버에 크롤링 하고자 하는 웹페이지 요청/응답
3. 내려 받은 웹 문서의 xml 파일 분석하고 필요한 자료 수집
4. 여러 웹페이지를 대상으로 2와 3의 단계를 반복하며 데이터 수집/정리
